{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPnu0hVFqNZsGsCKEnovqx6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Automatic Differentiation**"],"metadata":{"id":"oV_kcG6Y1T8L"}},{"cell_type":"markdown","source":["In this notebook, I'm going to demonstrate GradientTape(), a function that implements a computation graph, to automate the evaluation of partial derivatives. Here, we define a function, y = x^2, we define a gradientTape, and then we call the gradient() function to evaluate the first derivative of that function with respect to one parameter of that function, in this case x. Note that a tf.Variable() is a tensor that can hold mutable (changeable) values. Here I have defined x as a scalar."],"metadata":{"id":"3WUDvR031aD_"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uo86Hd5d0756","executionInfo":{"status":"ok","timestamp":1679594410857,"user_tz":240,"elapsed":6253,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"39a05c20-e181-4275-eafc-91b30e770bb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["The first derivative of x^2 is 6.0\n"]}],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","x = tf.Variable(3.0)\n","\n","with tf.GradientTape() as tape:\n","  y = x**2\n","\n","# f(x) = x^2; f'(x) = 2x; numerical evaluation of 2x when x = 3 is 2*3 = 6.\n","print(f'The first derivative of x^2 is {tape.gradient(y,x).numpy()}')"]},{"cell_type":"markdown","source":["We can also nest tapes to take a second derivative, like so:"],"metadata":{"id":"u0E6-nJ9JVkE"}},{"cell_type":"code","source":["with tf.GradientTape() as tape2:\n","  # If we don't specify 'persistent = True' then the tape is dropped from memory after the first gradient is evaluated.\n","  with tf.GradientTape(persistent=True) as tape:\n","     y=x**2\n","\n","  dy_dx = tape.gradient(y,x)\n","  print(f'The first derivative of x^2 is 2x; 2 * 3 = {dy_dx.numpy()}')\n","\n","d2y_dx = tape2.gradient(dy_dx,x)\n","\n","print(f'The second derivative of x^2 is {d2y_dx.numpy()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UhBUhCK3JY6f","executionInfo":{"status":"ok","timestamp":1679516476906,"user_tz":240,"elapsed":193,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"08f30811-841f-4bfb-c166-19ced777ed53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The first derivative of x^2 is 2x; 2 * 3 = 6.0\n","The second derivative of x^2 is 2.0\n"]}]},{"cell_type":"markdown","source":["We can also use tapes with higher dimensional tensors, e.g., here is an example for a sigmoid neuron. In the code below, note that w@x is tensor multiplication. Be careful with the order of the matrices and their dimensions; the matrix multiplication will only work if the shapes align. Also note that reduce_mean() returns the average over elements of y. We can directly obtain the gradient of the loss function w.r.t. our w and b parameters. The resulting gradients will be the same shape as the argument we are taking gradient with respect to, i.e., w.shape and b.shape. So, when we update values in a back pass, we can just calculate w -= dl_dw * learning_rate. and b -= dl_db * learning_rate."],"metadata":{"id":"O0Rx9a5M3iK5"}},{"cell_type":"code","source":["w = tf.Variable(tf.random.normal((3,1),dtype=tf.float32),name='w')\n","b = tf.Variable(tf.ones(1,dtype=tf.float32),name='b')\n","x = tf.constant([[1., 2., 5.]])\n","\n","# x is a 1x3 vector; w is a 3x1 matrix. So, 1x3 * 3*1 = 1x1 result, and adding a 1x1 bias (scalar) gives coherent output of the same shape.\n","print(f\"The shape of x*w+b will be: {(x@w+b).numpy().shape}\\n\")\n","\n","with tf.GradientTape() as tape:\n","  y = x @ w + b\n","  z = tf.sigmoid(y)\n","  # how am I operationalizing error here; what is my implicit ground truth?\n","  loss = tf.reduce_mean(z)\n","\n","[dl_dw,dl_db] = tape.gradient(loss,[w,b])\n","print(f'The gradients of loss w.r.t. weights is\\n{dl_dw}\\n The gradient w.r.t. b is {dl_db}.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g2gu5SDC3mCn","executionInfo":{"status":"ok","timestamp":1679516579516,"user_tz":240,"elapsed":4,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"abada454-a83d-4bad-98be-2b7edc432240"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The shape of x*w+b will be: (1, 1)\n","\n","The gradients of loss w.r.t. weights is\n","[[0.00179183]\n"," [0.00358366]\n"," [0.00895916]]\n"," The gradient w.r.t. b is [0.00179183].\n"]}]},{"cell_type":"markdown","source":["Let's do the same thing, but using a dictionary instead of a list..."],"metadata":{"id":"yUHzDpje-p-J"}},{"cell_type":"code","source":["with tf.GradientTape() as tape:\n","  y = x @ w + b\n","  z = tf.sigmoid(y)\n","  loss = tf.reduce_mean(z)\n","\n","my_parms = {\n","    'w': w,\n","    'b': b\n","}\n","\n","gradients = tape.gradient(loss,my_parms)\n","gradients['w']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awtrxduf-uhf","executionInfo":{"status":"ok","timestamp":1679516618812,"user_tz":240,"elapsed":121,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"fc8ad578-232a-4ece-cfad-f08f67387192"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n","array([[0.00179183],\n","       [0.00358366],\n","       [0.00895916]], dtype=float32)>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["#**Gradient Descent with a Simple Neural Network**"],"metadata":{"id":"WBOW960YYoDp"}},{"cell_type":"markdown","source":["Okay, now let's apply GradientTape to a mock neural network. We have our input layer of x's (4 features), and then a 3-node hidden layer employing ReLU activations, followed by a 4-node hidden layer employing tanh activations, followed by a single output node, employing sigmoid, for a binary DV. Our loss function will be the average of squared predictions. Note that this isn't really a meaningful NN; it's just for show. This definition means we have 3*2 = 6 weights and 2 bias terms."],"metadata":{"id":"U-mXbcgs_0Zy"}},{"cell_type":"code","source":["# Dense implements the operation: output = activation(dot(input, weights) + bias).\n","# Units = 2 means we have two nodes in the layer.\n","# We have 4 inputs, a hidden layer with 2 nodes, thus 4x2 = 8 weights, randomly initialized;\n","# The vector of 2 bias terms will initially default to values of 0.\n","# We then have a sigmoid output layer, e.g., a binary classification scenario, which takes 2 inputs, thus 2*1 = 2 weights, 1 bias term.\n","model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(units=2, activation=\"relu\", name=\"hiddenLayer\"),\n","        tf.keras.layers.Dense(units=1, activation=\"linear\",name=\"outputLayer\"),\n","])\n","\n","# Here is a random vector of 4 values, e.g., representing a single training observation.\n","input = tf.random.normal((1,4))"],"metadata":{"id":"8rr-fFhN_4dH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = model(input)\n","model.trainable_variables"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2uAAA8boRJt","executionInfo":{"status":"ok","timestamp":1679594494499,"user_tz":240,"elapsed":3,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"3ccca09b-b356-4be6-957e-8938e82f85ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Variable 'hiddenLayer/kernel:0' shape=(4, 2) dtype=float32, numpy=\n"," array([[ 0.8260925 ,  0.61075664],\n","        [-0.12604976, -0.58342147],\n","        [ 0.30424738,  0.37729406],\n","        [ 0.01328421,  0.27385664]], dtype=float32)>,\n"," <tf.Variable 'hiddenLayer/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n"," <tf.Variable 'outputLayer/kernel:0' shape=(2, 1) dtype=float32, numpy=\n"," array([[-0.86553544],\n","        [ 0.52753055]], dtype=float32)>,\n"," <tf.Variable 'outputLayer/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["Now we can apply GradientTape, in a loop, conducting a forward pass and then recovering gradients for all model parameters, and finally updating our parameters in the opposite direction from the gradients. We will iterate and repeat this process 100 times. Note, if we were doing this with N training examples, we would instead take the average loss across them. So, loss would equal tf.reduce_mean((prediction-target)**2)). We are sticking with a single training data point here for simplicity's sake."],"metadata":{"id":"gyUju8YLMjH0"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Define our learning rate and our ground truth value for our single training observation.\n","learning_rate = 1e-3\n","target=15\n","\n","history = []\n","for i in range(120):\n","  with tf.GradientTape() as tape:\n","    # Forward pass\n","    prediction = model(input)\n","    # We define our loss as the square of the forward pass prediction - 3.\n","    # 'target' is the true value, and we are taking the squared loss with respect to that value.\n","    loss = (prediction-target)**2\n","\n","  # Gradients with respect to every trainable variable, i.e., backward pass.\n","  grad = tape.gradient(loss, model.trainable_variables)\n","  for i in range(len(model.trainable_variables)):\n","    new_parms = model.trainable_variables[i] - grad[i]*learning_rate\n","    model.trainable_variables[i].assign(new_parms)\n","\n","  history.append(prediction.numpy())\n","\n","# Finally, collapse the list of arrays into a single array.\n","history = np.concatenate(history, axis=0)"],"metadata":{"id":"UrbO4KN-MjkW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we can plot the optimization process, showing how our model gradually improves until it predicts the correct value."],"metadata":{"id":"ZNGYTEtzThLp"}},{"cell_type":"code","source":["# Plot the evolution of our predictions as we tweak our network's weights and bias.\n","# Eventually, it converges to the correct prediction.\n","plt.plot(range(120),history,[target]*120)\n","plt.legend(('Predicted','True'))\n","plt.xlabel('Iteration')\n","plt.ylabel('y value')\n","plt.title('Optimization of a Simple NN')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"UEERAnfWTlX-","executionInfo":{"status":"ok","timestamp":1679516774653,"user_tz":240,"elapsed":412,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"61cb91cd-6d94-4ecb-a41f-d9b32e0d6313"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Optimization of a Simple NN')"]},"metadata":{},"execution_count":10},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["We can see what the final parameter values are in the fitted model..."],"metadata":{"id":"8NVMV8AbA9LB"}},{"cell_type":"code","source":["model.layers[0].get_weights()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EulJDpLZAVj1","executionInfo":{"status":"ok","timestamp":1679516993522,"user_tz":240,"elapsed":160,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"7a34e5d8-61c3-4172-f3aa-5a6a18d4fd7f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[-1.1726863 , -1.2067394 ],\n","        [ 0.23990153, -0.81552476],\n","        [-0.5164312 , -0.45791358],\n","        [ 1.5685458 ,  1.1876186 ]], dtype=float32),\n"," array([0.4360297 , 0.42928943], dtype=float32)]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["model.layers[1].get_weights()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULgcKL4wA6J_","executionInfo":{"status":"ok","timestamp":1679517005427,"user_tz":240,"elapsed":189,"user":{"displayName":"Gordon Burtch","userId":"10144756805379529333"}},"outputId":"0b1990bc-ee88-46bd-e66b-666f699fd6ef"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[1.9490174],\n","        [1.8534218]], dtype=float32), array([0.2908994], dtype=float32)]"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["#**Gradient Tape Exercise**"],"metadata":{"id":"1fnbHC20LxJ2"}},{"cell_type":"markdown","source":["Use tf.GradientTape() to plot the Hyperbolic Tangent activation function over input values ranging from -10 to 10, and overlay a plot of the first derivative of the activation function.\n","\n","Notes:\n","\n","* *Any variable you want to calculate a derivative with respect to should be declared as a tf.Variable().*\n","* *To convert a tf tensor into a numpy array, apply the .numpy() function to it.*\n","* *The tanh function is pre-defined for you in tf.nn.tanh(), as are some other activation functions.*"],"metadata":{"id":"69fQkLUSL4Ha"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Simulating some data for you to initialize x with. This is just a 200x1 array of random values between -10 and +10, sorted.\n","data = tf.sort(tf.random.uniform((200,1),minval=-10,maxval=10),axis=0)\n","\n","x = ## Declare your variable.\n","\n","with # Declare your tape\n","  y = ## Write your activation function here.\n","\n","dy_dx = # Obtain gradient across values of x, from the tape, here.\n","\n","# Create your plot\n","plt.plot()# arguments here are x and y)\n","plt.plot()# arguments here are x and the gradient values)\n","plt.legend(['Tanh','Tanh\\''])\n","plt.show()"],"metadata":{"id":"fBo6mNhjMDD1"},"execution_count":null,"outputs":[]}]}